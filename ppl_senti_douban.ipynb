{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed  = 1\n",
    "from transformers import set_seed, BertTokenizer, TFAutoModelForMaskedLM, AutoTokenizer, get_scheduler, BertForPreTraining, BertForMaskedLM, BertConfig\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim import AdamW\n",
    "import argparse, torch, datasets, ast, operator, gc, os\n",
    "import torch.nn as nn\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def score(sentence,model):\n",
    "# #     if len(sentence)>128:\n",
    "# #         sentence = sentence[:128]\n",
    "#     with torch.no_grad():\n",
    "#         model.eval()\n",
    "#         tokenize_input = tokenizer.tokenize(sentence)\n",
    "#         tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "#         sen_len = len(tokenize_input)\n",
    "#         sentence_loss = 0.\n",
    "\n",
    "#         for i, word in enumerate(tokenize_input):\n",
    "#             # add mask to i-th character of the sentence\n",
    "#             tokenize_input[i] = '[MASK]'\n",
    "#             mask_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "            \n",
    "#             mask_input=mask_input.to(device)\n",
    "\n",
    "#             output = model(mask_input)\n",
    "\n",
    "#             prediction_scores = output[0]\n",
    "#             softmax = nn.Softmax(dim=0)\n",
    "#             ps = softmax(prediction_scores[0, i]).log()\n",
    "#             word_loss = ps[tensor_input[0, i]]\n",
    "#             word_loss_new = word_loss.detach().cpu().numpy()\n",
    "#             sentence_loss += word_loss.item()\n",
    "#             tokenize_input[i] = word\n",
    "#         ppl = np.exp(-sentence_loss/sen_len)\n",
    "#         del output,mask_input,ps,word_loss, prediction_scores,tensor_input,tokenize_input\n",
    "#         gc.collect()\n",
    "#         torch.cuda.empty_cache()\n",
    "#         return ppl\n",
    "    \n",
    "def compute_ppl(sentence,model):\n",
    "#     if len(sentence)>128:\n",
    "#         sentence = sentence[:128]\n",
    "    tokenize_input = tokenizer.tokenize(sentence)\n",
    "    candidates = []\n",
    "    target_words = []\n",
    "    for i in range(len(tokenize_input)):\n",
    "        temp = tokenize_input.copy()\n",
    "        # temp = tokenize_input.copy()[:i+1]\n",
    "        target_words.append(temp[i])\n",
    "        temp[i] = tokenizer.mask_token\n",
    "        candidates.append(temp)\n",
    "\n",
    "        \n",
    "    # print(candidates)\n",
    "    with torch.no_grad():\n",
    "        model.eval()     \n",
    "        sentence_loss = 0.\n",
    "\n",
    "        for i, (candidate, target_word) in enumerate(zip(candidates,target_words)):\n",
    "            \n",
    "            target_word_id = tokenizer.convert_tokens_to_ids(target_word)\n",
    "            tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(candidate)]).to(device)\n",
    "            output = model(tensor_input)\n",
    "            prediction_scores = output.logits\n",
    "            softmax = nn.Softmax(dim=0)\n",
    "            \n",
    "            ps = softmax(prediction_scores[0, i]).log()\n",
    "            word_loss = ps[target_word_id]\n",
    "            word_loss_new = word_loss.detach().cpu().numpy()\n",
    "            sentence_loss += word_loss_new.item()\n",
    "        #     print(word_loss_new)\n",
    "        \n",
    "\n",
    "        # print(sentence_loss)\n",
    "        # print(sentence_loss/len(candidates))\n",
    "        ppl = np.exp(-sentence_loss/len(candidates))\n",
    "        del output,ps,word_loss, prediction_scores,tensor_input,tokenize_input\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=2, datasets='./datasets/', max_len=512, model='./models/my_roberta/', result_output_dir='./ppl_results/', task='douban', tokenizer='./models/my_roberta/')\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='search for best template according to dev set')\n",
    "parser.add_argument('--max_len', default=512, type=int, help=\"max sequence length\")\n",
    "parser.add_argument('--batch_size', default=2, type=int, help=\"batch size\")\n",
    "parser.add_argument('--model', default='./models/my_roberta/', type=str, help=\"pretrained model\")\n",
    "parser.add_argument('--result_output_dir', default='./ppl_results/', type=str, help=\"output directory\")\n",
    "parser.add_argument('--tokenizer', default='./models/my_roberta/', type=str, help=\"tokenizer\")\n",
    "parser.add_argument('--task', default='douban', type=str, help=\"task name\")\n",
    "parser.add_argument('--datasets', default='./datasets/', type=str, help=\"dataset dir\")\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tokenizer ./models/my_roberta/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./models/my_roberta/ were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'loading tokenizer {args.tokenizer}')\n",
    "tokenizer = BertTokenizer.from_pretrained(f'{args.tokenizer}')\n",
    "pretrained_model = BertForMaskedLM.from_pretrained(args.model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_all = pd.read_csv(f'{args.datasets}{args.task}_output.csv',names=['movie_name','scores','text','labels'],header=0)\n",
    "texts = pd_all.text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1094/1094 [04:54<00:00,  3.72it/s]\n"
     ]
    }
   ],
   "source": [
    "ppl_scores = []\n",
    "for text in tqdm(texts):\n",
    "    score = compute_ppl(text,pretrained_model)\n",
    "    ppl_scores.append(score)\n",
    "\n",
    "pd_all['ppl'] = ppl_scores\n",
    "pd_all.to_csv(f'{args.datasets}{args.task}_ppl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
