{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "seed  = 1\n",
    "from transformers import set_seed, BertTokenizer, TFAutoModelForMaskedLM, AutoTokenizer, get_scheduler, BertForPreTraining, BertForMaskedLM, BertConfig\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim import AdamW\n",
    "import argparse, torch, datasets, ast, operator, gc, os\n",
    "import torch.nn as nn\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(dataset_dir='./datasets/', task='htl')\n",
      "数据集htl\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='search for best template according to dev set')\n",
    "\n",
    "# parser.add_argument('--model', default='../Proxy_Prompt_LM/models/my_roberta/', type=str, help=\"pretrained model\")\n",
    "parser.add_argument('--dataset_dir', default='./datasets/', type=str, help=\"dataset dir\")\n",
    "parser.add_argument('--task', default='htl', type=str, help=\"task name\")\n",
    "\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "print(args)\n",
    "print('''数据集'''+args.task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 27 128]\n"
     ]
    }
   ],
   "source": [
    "if args.task ==\"eprstmt\":\n",
    "    few_shot_data_path = f'./data/k-shot/0/{args.task}/16-13'\n",
    "    df_train = pd.read_csv(f'{few_shot_data_path}/test.csv',index_col=0,names=['text','labels'])\n",
    "elif args.task == 'weibo':\n",
    "    few_shot_data_path = f'{args.dataset_dir}weibo_senti_100k.csv'\n",
    "    pd_all = pd.read_csv(few_shot_data_path,names=['labels','text'],header=0)\n",
    "    pd_all = pd_all[\n",
    "    pd_all[\"text\"].apply(lambda x: len(x) >= 14 and len(x)<=15)\n",
    "    ]\n",
    "    \n",
    "    labels = pd_all.labels.tolist()\n",
    "    print(np.bincount(labels))\n",
    "    min_label = min(np.bincount(labels))\n",
    "    pd_pos = pd_all[pd_all.labels==1].sample(min_label)\n",
    "    pd_neg = pd_all[pd_all.labels==0].sample(min_label)\n",
    "    pd_output = pd.concat([pd_pos,pd_neg])\n",
    "    pd_output = pd_output.sample(frac=1)\n",
    "\n",
    "    print(pd_output)\n",
    "    pd_output.to_csv(f'{args.dataset_dir}weibo_output.csv')\n",
    "\n",
    "\n",
    "    \n",
    "    # pd_dev_pos = pd_all[pd_all.labels==1].sample(250)\n",
    "    # pd_dev_neg = pd_all[pd_all.labels==0].sample(250)   \n",
    "    # pd_dev = pd.concat([pd_dev_pos,pd_dev_neg])\n",
    "    # pd_dev = pd_dev.sample(frac=1)\n",
    "    # pd_dev.to_csv(f'{args.dataset_dir}weibo_dev.csv')\n",
    "    \n",
    "    # pd_all = pd_all.drop(pd_dev.index)\n",
    "    # print(len(pd_all))\n",
    "    \n",
    "    # pd_test_pos = pd_all[pd_all.labels==1].sample(250)\n",
    "    # pd_test_neg = pd_all[pd_all.labels==0].sample(250)   \n",
    "    # pd_test = pd.concat([pd_test_pos,pd_test_neg])\n",
    "    # pd_test = pd_test.sample(frac=1)\n",
    "    # pd_test.to_csv(f'{args.dataset_dir}weibo_test.csv')\n",
    "    \n",
    "    # pd_all = pd_all.drop(pd_test.index)\n",
    "    # print(len(pd_all))\n",
    "    \n",
    "    # pd_all.to_csv(f'{args.dataset_dir}weibo_train.csv')\n",
    "    \n",
    "    # for seed in range(0,5):\n",
    "    #     set_seed(seed)\n",
    "    #     pd_train_pos = pd_all[pd_all.labels==1].sample(16)\n",
    "    #     pd_train_neg = pd_all[pd_all.labels==0].sample(16)   \n",
    "    #     pd_train = pd.concat([pd_train_pos,pd_train_neg])\n",
    "    #     pd_train = pd_train.sample(frac=1)\n",
    "    #     print(len(pd_train))\n",
    "    #     pd_all.to_csv(f'{args.dataset_dir}weibo_split_{seed}.csv')\n",
    "    \n",
    "elif args.task =='waimai':\n",
    "    few_shot_data_path = f'{args.dataset_dir}waimai_10k.csv'\n",
    "    pd_all = pd.read_csv(few_shot_data_path,names=['labels','text'],header=0)\n",
    "    pd_all = pd_all[\n",
    "    pd_all[\"text\"].apply(lambda x: len(x) >= 14 and len(x)<=15)\n",
    "    ]\n",
    "    labels = pd_all.labels.tolist()\n",
    "    print(np.bincount(labels))\n",
    "    min_label = min(np.bincount(labels))\n",
    "    pd_pos = pd_all[pd_all.labels==1].sample(min_label)\n",
    "    pd_neg = pd_all[pd_all.labels==0].sample(min_label)\n",
    "    pd_output = pd.concat([pd_pos,pd_neg])\n",
    "    pd_output = pd_output.sample(frac=1)\n",
    "\n",
    "    print(pd_output)\n",
    "    pd_output.to_csv(f'{args.dataset_dir}waimai_output.csv')\n",
    "    \n",
    "    # pd_dev_pos = pd_all[pd_all.labels==1].sample(250)\n",
    "    # pd_dev_neg = pd_all[pd_all.labels==0].sample(250)   \n",
    "    # pd_dev = pd.concat([pd_dev_pos,pd_dev_neg])\n",
    "    # pd_dev = pd_dev.sample(frac=1)\n",
    "    # pd_dev.to_csv(f'{args.dataset_dir}waimai_dev.csv')\n",
    "    \n",
    "    # pd_all = pd_all.drop(pd_dev.index)\n",
    "    # print(len(pd_all))\n",
    "    \n",
    "    # pd_test_pos = pd_all[pd_all.labels==1].sample(250)\n",
    "    # pd_test_neg = pd_all[pd_all.labels==0].sample(250)   \n",
    "    # pd_test = pd.concat([pd_test_pos,pd_test_neg])\n",
    "    # pd_test = pd_test.sample(frac=1)\n",
    "    # pd_test.to_csv(f'{args.dataset_dir}waimai_test.csv')\n",
    "    \n",
    "    # pd_all = pd_all.drop(pd_test.index)\n",
    "    # print(len(pd_all))\n",
    "    \n",
    "    # pd_all.to_csv(f'{args.dataset_dir}waimai_train.csv')\n",
    "    \n",
    "    # for seed in range(0,5):\n",
    "    #     set_seed(seed)\n",
    "    #     pd_train_pos = pd_all[pd_all.labels==1].sample(16)\n",
    "    #     pd_train_neg = pd_all[pd_all.labels==0].sample(16)   \n",
    "    #     pd_train = pd.concat([pd_train_pos,pd_train_neg])\n",
    "    #     pd_train = pd_train.sample(frac=1)\n",
    "    #     print(len(pd_train))\n",
    "    #     pd_all.to_csv(f'{args.dataset_dir}waimai_split_{seed}.csv')\n",
    "elif args.task=='ecommerce':\n",
    "    few_shot_data_path = f'{args.dataset_dir}online_shopping_10_cats.csv'\n",
    "    pd_all = pd.read_csv(few_shot_data_path,names=['cat','labels','text'],header=0,index_col=False)\n",
    "    pd_all = pd_all[\n",
    "    pd_all[\"text\"].apply(lambda x: len(str(x)) >= 14 and len(str(x))<=15)\n",
    "    ]\n",
    "    labels = pd_all.labels.tolist()\n",
    "    print(np.bincount(labels))\n",
    "    min_label = min(np.bincount(labels))\n",
    "    pd_pos = pd_all[pd_all.labels==1].sample(min_label)\n",
    "    pd_neg = pd_all[pd_all.labels==0].sample(min_label)\n",
    "    pd_output = pd.concat([pd_pos,pd_neg])\n",
    "    pd_output = pd_output.sample(frac=1)\n",
    "\n",
    "    print(pd_output)\n",
    "    pd_output.to_csv(f'{args.dataset_dir}ecommerce_output.csv')\n",
    "    \n",
    "    # pd_dev_pos = pd_all[pd_all.labels==1].sample(1000)\n",
    "    # pd_dev_neg = pd_all[pd_all.labels==0].sample(1000)   \n",
    "    # pd_dev = pd.concat([pd_dev_pos,pd_dev_neg])\n",
    "    # pd_dev = pd_dev.sample(frac=1)\n",
    "    # pd_dev.to_csv(f'{args.dataset_dir}ecommerce_dev.csv')\n",
    "    \n",
    "    # pd_all = pd_all.drop(pd_dev.index)\n",
    "    # print(len(pd_all))\n",
    "    \n",
    "    # pd_test_pos = pd_all[pd_all.labels==1].sample(2000)\n",
    "    # pd_test_neg = pd_all[pd_all.labels==0].sample(2000)   \n",
    "    # pd_test = pd.concat([pd_test_pos,pd_test_neg])\n",
    "    # pd_test = pd_test.sample(frac=1)\n",
    "    # pd_test.to_csv(f'{args.dataset_dir}ecommerce_test.csv')\n",
    "    \n",
    "    # pd_all = pd_all.drop(pd_test.index)\n",
    "    # print(len(pd_all))\n",
    "    \n",
    "    # pd_all.to_csv(f'{args.dataset_dir}ecommerce_train.csv')\n",
    "    \n",
    "    # for seed in range(0,5):\n",
    "    #     set_seed(seed)\n",
    "    #     pd_train_pos = pd_all[pd_all.labels==1].sample(16)\n",
    "    #     pd_train_neg = pd_all[pd_all.labels==0].sample(16)   \n",
    "    #     pd_train = pd.concat([pd_train_pos,pd_train_neg])\n",
    "    #     pd_train = pd_train.sample(frac=1)\n",
    "    #     print(len(pd_train))\n",
    "    #     pd_all.to_csv(f'{args.dataset_dir}ecommerce_split_{seed}.csv')\n",
    "elif args.task=='douban':\n",
    "    few_shot_data_path = f'{args.dataset_dir}douban_comments.csv'\n",
    "    pd_all = pd.read_csv(few_shot_data_path,names=['movie_name','scores','text','labels'],header=0,index_col=0)\n",
    "    print(len(pd_all))\n",
    "    pd_all = pd_all[\n",
    "    pd_all[\"text\"].apply(lambda x: len(str(x)) >= 14 and len(str(x))<=15)\n",
    "    ]\n",
    "    labels = pd_all.labels.tolist()\n",
    "    print(np.bincount(labels))\n",
    "    min_label = min(np.bincount(labels))\n",
    "    pd_pos = pd_all[pd_all.labels==1].sample(min_label)\n",
    "    pd_neg = pd_all[pd_all.labels==0].sample(min_label)\n",
    "    pd_output = pd.concat([pd_pos,pd_neg])\n",
    "    pd_output = pd_output.sample(frac=1)\n",
    "\n",
    "    print(pd_output)\n",
    "    pd_output.to_csv(f'{args.dataset_dir}douban_output.csv')\n",
    "    \n",
    "    # pd_dev_pos = pd_all[pd_all.labels==1].sample(1000)\n",
    "    # pd_dev_neg = pd_all[pd_all.labels==0].sample(1000)   \n",
    "    # pd_dev = pd.concat([pd_dev_pos,pd_dev_neg])\n",
    "    # pd_dev = pd_dev.sample(frac=1)\n",
    "    # pd_dev.to_csv(f'{args.dataset_dir}douban_dev.csv')\n",
    "    \n",
    "    # pd_all = pd_all.drop(pd_dev.index)\n",
    "    # print(len(pd_all))\n",
    "    \n",
    "    # pd_test_pos = pd_all[pd_all.labels==1].sample(2000)\n",
    "    # pd_test_neg = pd_all[pd_all.labels==0].sample(2000)   \n",
    "    # pd_test = pd.concat([pd_test_pos,pd_test_neg])\n",
    "    # pd_test = pd_test.sample(frac=1)\n",
    "    # pd_test.to_csv(f'{args.dataset_dir}douban_test.csv')\n",
    "    \n",
    "    # pd_all = pd_all.drop(pd_test.index)\n",
    "    # print(len(pd_all))\n",
    "    \n",
    "    # pd_all.to_csv(f'{args.dataset_dir}douban_train.csv')\n",
    "    \n",
    "    # for seed in range(0,5):\n",
    "    #     set_seed(seed)\n",
    "    #     pd_train_pos = pd_all[pd_all.labels==1].sample(16)\n",
    "    #     pd_train_neg = pd_all[pd_all.labels==0].sample(16)   \n",
    "    #     pd_train = pd.concat([pd_train_pos,pd_train_neg])\n",
    "    #     pd_train = pd_train.sample(frac=1)\n",
    "    #     print(len(pd_train))\n",
    "    #     pd_all.to_csv(f'{args.dataset_dir}douban_split_{seed}.csv')\n",
    "\n",
    "elif args.task=='htl':\n",
    "    few_shot_data_path = f'{args.dataset_dir}htl_7k.csv'\n",
    "    pd_all = pd.read_csv(few_shot_data_path,names=['labels','text'],header=0,index_col=False)\n",
    "    pd_all = pd_all[\n",
    "    pd_all[\"text\"].apply(lambda x: len(str(x)) >= 24 and len(str(x))<=25)\n",
    "    ]\n",
    "    labels = pd_all.labels.tolist()\n",
    "    print(np.bincount(labels))\n",
    "    min_label = min(np.bincount(labels))\n",
    "    pd_pos = pd_all[pd_all.labels==1].sample(min_label)\n",
    "    pd_neg = pd_all[pd_all.labels==0].sample(min_label)\n",
    "    pd_output = pd.concat([pd_pos,pd_neg])\n",
    "    pd_output = pd_output.sample(frac=1)\n",
    "\n",
    "    pd_output.to_csv(f'{args.dataset_dir}htl_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_name</th>\n",
       "      <th>scores</th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>七宗罪</td>\n",
       "      <td>3.0</td>\n",
       "      <td>因为看的年代晚了所以惊喜全无～</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>七宗罪</td>\n",
       "      <td>5.0</td>\n",
       "      <td>为什么拍出来十几年我才看到呢？</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>七宗罪</td>\n",
       "      <td>5.0</td>\n",
       "      <td>结局真的很惊人。惨绝人寰……</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1361</th>\n",
       "      <td>七宗罪</td>\n",
       "      <td>5.0</td>\n",
       "      <td>里面的布拉德皮特帅的掉渣……</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>七月与安生</td>\n",
       "      <td>2.0</td>\n",
       "      <td>年龄大了，这种友谊想着都头疼。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50548</th>\n",
       "      <td>龙威父子</td>\n",
       "      <td>1.0</td>\n",
       "      <td>年轻不懂事的时候看过。。。。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50549</th>\n",
       "      <td>龙威父子</td>\n",
       "      <td>1.0</td>\n",
       "      <td>弱智版的断水流大师兄与何金银</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50552</th>\n",
       "      <td>龙威父子</td>\n",
       "      <td>1.0</td>\n",
       "      <td>洪大这样的作品真是让人大跌眼镜</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50567</th>\n",
       "      <td>龙威父子</td>\n",
       "      <td>2.0</td>\n",
       "      <td>原来黄晓明很早的时候就负责二了</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50569</th>\n",
       "      <td>龙威父子</td>\n",
       "      <td>2.0</td>\n",
       "      <td>基本反映了黄晓明的表演功力。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1512 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      movie_name  scores             text  labels\n",
       "1167         七宗罪     3.0  因为看的年代晚了所以惊喜全无～       0\n",
       "1284         七宗罪     5.0  为什么拍出来十几年我才看到呢？       1\n",
       "1340         七宗罪     5.0   结局真的很惊人。惨绝人寰……       1\n",
       "1361         七宗罪     5.0   里面的布拉德皮特帅的掉渣……       1\n",
       "1395       七月与安生     2.0  年龄大了，这种友谊想着都头疼。       0\n",
       "...          ...     ...              ...     ...\n",
       "50548       龙威父子     1.0   年轻不懂事的时候看过。。。。       0\n",
       "50549       龙威父子     1.0   弱智版的断水流大师兄与何金银       0\n",
       "50552       龙威父子     1.0  洪大这样的作品真是让人大跌眼镜       0\n",
       "50567       龙威父子     2.0  原来黄晓明很早的时候就负责二了       0\n",
       "50569       龙威父子     2.0   基本反映了黄晓明的表演功力。       0\n",
       "\n",
       "[1512 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "douban = pd.read_csv('./datasets/douban_output.csv',names=['movie_name','scores','text','labels'],header=0,index_col=0)\n",
    "del douban['movie_name'], douban['scores']\n",
    "douban = douban[[\"labels\", \"text\"]]\n",
    "douban.to_csv(f'{args.dataset_dir}douban_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecommerce = pd.read_csv('./datasets/ecommerce_output.csv',names=['cat','labels','text'],header=0,index_col=0)\n",
    "del ecommerce['cat']\n",
    "ecommerce.to_csv(f'{args.dataset_dir}ecommerce_output.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
